{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8945500,"sourceType":"datasetVersion","datasetId":5356391},{"sourceId":8945503,"sourceType":"datasetVersion","datasetId":5356424}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example,here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing,CSV file I/O (e.g. pd.read_csv)\nimport re\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(44) # better for debugging to have seed \nimport time\nimport datetime\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example,running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname,_,filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname,filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/,but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-13T15:15:34.777186Z","iopub.execute_input":"2024-07-13T15:15:34.777810Z","iopub.status.idle":"2024-07-13T15:15:39.475034Z","shell.execute_reply.started":"2024-07-13T15:15:34.777775Z","shell.execute_reply":"2024-07-13T15:15:39.473997Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/arabic-poetry/arabic-poems.txt\n/kaggle/input/tiny-arabic-poetry/small-arabic-poems.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('../input/tiny-arabic-poetry/small-arabic-poems.txt','r',encoding='utf-8') as f:\n    poems=f.read()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:39.477090Z","iopub.execute_input":"2024-07-13T15:15:39.477682Z","iopub.status.idle":"2024-07-13T15:15:39.735598Z","shell.execute_reply.started":"2024-07-13T15:15:39.477644Z","shell.execute_reply":"2024-07-13T15:15:39.734739Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(\"Number of characters in dataset=\",len(poems))","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:39.740336Z","iopub.execute_input":"2024-07-13T15:15:39.740600Z","iopub.status.idle":"2024-07-13T15:15:39.745724Z","shell.execute_reply.started":"2024-07-13T15:15:39.740577Z","shell.execute_reply":"2024-07-13T15:15:39.744875Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Number of characters in dataset= 10000016\n","output_type":"stream"}]},{"cell_type":"code","source":"poems[:1000]","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:39.746820Z","iopub.execute_input":"2024-07-13T15:15:39.747063Z","iopub.status.idle":"2024-07-13T15:15:39.758248Z","shell.execute_reply.started":"2024-07-13T15:15:39.747040Z","shell.execute_reply":"2024-07-13T15:15:39.757397Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'بدت تختال في حلل الجمال\\nوجادت بالزيارة والوصال\\nتميس فلا يعادلها قضيب\\nوإن ترنو تداعب بالنصال\\nبمبسمها لعمر أبيك در\\nوفي أعماقها نبع الزلال\\nوخصر يستبيك إذا تولت\\nكغصن البان في كثب الرمال\\nتبدت كالقضيب على كثيب\\nوجلت كالمنيرة في الليالي\\nفقمت أداعب الوجنات منها\\nوألثم ثغرها حكي اللآلي\\nوأهصر غصنها ضما ولما\\nوألهو باليمين وبالشمال\\nومن خلق العفاف لنا رقيب\\nبطهر الحب في حسن الخلال\\nوقد غاب الرقيب وطاب أنسي\\nوطير الحب يصدح بامتثال\\nتقول أراك تظهر لي اشتياقا\\nوتفعل كالمودع للرحال\\nفقلت لها رويدك إن قلبي\\nوحبي لم يكن يوما بسالي\\nولكني عزمت وفي عزم\\nليهزأ بالأسنة والعوالي\\nسئمت من المقام وكل شيء\\nإذا ما دام يسأم لا محال\\nسأضرب في الحياة بسهم جدي\\nونحظى بالمسرة والوصال\\nوأرجع إن يشا الباري قريبا\\nإليك لنبتني صرح المعالي\\nفرقرق لؤلؤ في مقلتيها\\nوصاحت آه من مر الليالي\\nأتترك يا حبيب الروح قلبا\\nيكاد يذوب من شوق لحالي\\nوتسلوني وأنت نعيم روحي\\nولا ترعى المودة أو تبالي\\nليحفظ الإله بكل أرض\\nويرزقك السلامة في الكمال\\nوجمنا لم نحر قولا ولكن\\nنهير الدمع فاض على التوالي\\nولاح الصبح من تحت الثريا\\nكما لاح المشيب بعين قال\\nفيالله كم ذابت قلوب\\n'"},"metadata":{}}]},{"cell_type":"code","source":"# gather all unique characters within dataset\nchars=sorted(list(set(poems)))\nvocab_size=len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:39.759338Z","iopub.execute_input":"2024-07-13T15:15:39.759687Z","iopub.status.idle":"2024-07-13T15:15:40.315985Z","shell.execute_reply.started":"2024-07-13T15:15:39.759657Z","shell.execute_reply":"2024-07-13T15:15:40.314738Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\t\n ؟ءآأؤإئابةتثجحخدذرزسشصضطظعغـفقكلمنهوىيپچ  \n45\n","output_type":"stream"}]},{"cell_type":"code","source":"# tokenize chars by mapping to integers\nstoi={ch:i for i,ch in enumerate(chars)}\nitos={i:ch for i,ch in enumerate(chars)}\nencode=lambda s:[stoi[c] for c in s] # string --> list of integers\ndecode=lambda l:''.join([itos[i] for i in l]) # list of integers --> string","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:40.317289Z","iopub.execute_input":"2024-07-13T15:15:40.317588Z","iopub.status.idle":"2024-07-13T15:15:40.329590Z","shell.execute_reply.started":"2024-07-13T15:15:40.317564Z","shell.execute_reply":"2024-07-13T15:15:40.328781Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\n# encode dataset\ndata=torch.tensor(encode(poems),dtype=torch.long)\nprint(data.shape,data.dtype)\nprint(data[:1000])","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:40.330622Z","iopub.execute_input":"2024-07-13T15:15:40.330885Z","iopub.status.idle":"2024-07-13T15:15:42.801805Z","shell.execute_reply.started":"2024-07-13T15:15:40.330862Z","shell.execute_reply":"2024-07-13T15:15:42.800865Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"torch.Size([10000016]) torch.int64\ntensor([11, 18, 13,  2, 13, 17, 13, 10, 34,  2, 31, 40,  2, 16, 34, 34,  2, 10,\n        34, 15, 35, 10, 34,  1, 38, 15, 10, 18, 13,  2, 11, 10, 34, 21, 40, 10,\n        20, 12,  2, 38, 10, 34, 38, 24, 10, 34,  1, 13, 35, 40, 22,  2, 31, 34,\n        10,  2, 40, 28, 10, 18, 34, 37, 10,  2, 32, 25, 40, 11,  1, 38,  8, 36,\n         2, 13, 20, 36, 38,  2, 13, 18, 10, 28, 11,  2, 11, 10, 34, 36, 24, 10,\n        34,  1, 11, 35, 11, 22, 35, 37, 10,  2, 34, 28, 35, 20,  2,  6, 11, 40,\n        33,  2, 18, 20,  1, 38, 31, 40,  2,  6, 28, 35, 10, 32, 37, 10,  2, 36,\n        11, 28,  2, 10, 34, 21, 34, 10, 34,  1, 38, 17, 24, 20,  2, 40, 22, 13,\n        11, 40, 33,  2,  8, 19, 10,  2, 13, 38, 34, 13,  1, 33, 29, 24, 36,  2,\n        10, 34, 11, 10, 36,  2, 31, 40,  2, 33, 14, 11,  2, 10, 34, 20, 35, 10,\n        34,  1, 13, 11, 18, 13,  2, 33, 10, 34, 32, 25, 40, 11,  2, 28, 34, 39,\n         2, 33, 14, 40, 11,  1, 38, 15, 34, 13,  2, 33, 10, 34, 35, 36, 40, 20,\n        12,  2, 31, 40,  2, 10, 34, 34, 40, 10, 34, 40,  1, 31, 32, 35, 13,  2,\n         6, 18, 10, 28, 11,  2, 10, 34, 38, 15, 36, 10, 13,  2, 35, 36, 37, 10,\n         1, 38,  6, 34, 14, 35,  2, 14, 29, 20, 37, 10,  2, 16, 33, 40,  2, 10,\n        34, 34,  5, 34, 40,  1, 38,  6, 37, 24, 20,  2, 29, 24, 36, 37, 10,  2,\n        25, 35, 10,  2, 38, 34, 35, 10,  1, 38,  6, 34, 37, 38,  2, 11, 10, 34,\n        40, 35, 40, 36,  2, 38, 11, 10, 34, 23, 35, 10, 34,  1, 38, 35, 36,  2,\n        17, 34, 32,  2, 10, 34, 28, 31, 10, 31,  2, 34, 36, 10,  2, 20, 32, 40,\n        11,  1, 11, 26, 37, 20,  2, 10, 34, 16, 11,  2, 31, 40,  2, 16, 22, 36,\n         2, 10, 34, 17, 34, 10, 34,  1, 38, 32, 18,  2, 29, 10, 11,  2, 10, 34,\n        20, 32, 40, 11,  2, 38, 26, 10, 11,  2,  6, 36, 22, 40,  1, 38, 26, 40,\n        20,  2, 10, 34, 16, 11,  2, 40, 24, 18, 16,  2, 11, 10, 35, 13, 14, 10,\n        34,  1, 13, 32, 38, 34,  2,  6, 20, 10, 33,  2, 13, 27, 37, 20,  2, 34,\n        40,  2, 10, 23, 13, 40, 10, 32, 10,  1, 38, 13, 31, 28, 34,  2, 33, 10,\n        34, 35, 38, 18, 28,  2, 34, 34, 20, 16, 10, 34,  1, 31, 32, 34, 13,  2,\n        34, 37, 10,  2, 20, 38, 40, 18, 33,  2,  8, 36,  2, 32, 34, 11, 40,  1,\n        38, 16, 11, 40,  2, 34, 35,  2, 40, 33, 36,  2, 40, 38, 35, 10,  2, 11,\n        22, 10, 34, 40,  1, 38, 34, 33, 36, 40,  2, 28, 21, 35, 13,  2, 38, 31,\n        40,  2, 28, 21, 35,  1, 34, 40, 37, 21,  6,  2, 11, 10, 34,  6, 22, 36,\n        12,  2, 38, 10, 34, 28, 38, 10, 34, 40,  1, 22,  9, 35, 13,  2, 35, 36,\n         2, 10, 34, 35, 32, 10, 35,  2, 38, 33, 34,  2, 23, 40,  4,  1,  8, 19,\n        10,  2, 35, 10,  2, 18, 10, 35,  2, 40, 22,  6, 35,  2, 34, 10,  2, 35,\n        16, 10, 34,  1, 22,  6, 25, 20, 11,  2, 31, 40,  2, 10, 34, 16, 40, 10,\n        12,  2, 11, 22, 37, 35,  2, 15, 18, 40,  1, 38, 36, 16, 27, 39,  2, 11,\n        10, 34, 35, 22, 20, 12,  2, 38, 10, 34, 38, 24, 10, 34,  1, 38,  6, 20,\n        15, 28,  2,  8, 36,  2, 40, 23, 10,  2, 10, 34, 11, 10, 20, 40,  2, 32,\n        20, 40, 11, 10,  1,  8, 34, 40, 33,  2, 34, 36, 11, 13, 36, 40,  2, 24,\n        20, 16,  2, 10, 34, 35, 28, 10, 34, 40,  1, 31, 20, 32, 20, 32,  2, 34,\n         7, 34,  7,  2, 31, 40,  2, 35, 32, 34, 13, 40, 37, 10,  1, 38, 24, 10,\n        16, 13,  2,  5, 37,  2, 35, 36,  2, 35, 20,  2, 10, 34, 34, 40, 10, 34,\n        40,  1,  6, 13, 13, 20, 33,  2, 40, 10,  2, 16, 11, 40, 11,  2, 10, 34,\n        20, 38, 16,  2, 32, 34, 11, 10,  1, 40, 33, 10, 18,  2, 40, 19, 38, 11,\n         2, 35, 36,  2, 23, 38, 32,  2, 34, 16, 10, 34, 40,  1, 38, 13, 22, 34,\n        38, 36, 40,  2, 38,  6, 36, 13,  2, 36, 28, 40, 35,  2, 20, 38, 16, 40,\n         1, 38, 34, 10,  2, 13, 20, 28, 39,  2, 10, 34, 35, 38, 18, 12,  2,  6,\n        38,  2, 13, 11, 10, 34, 40,  1, 34, 40, 16, 31, 27,  2, 10, 34,  8, 34,\n        37,  2, 11, 33, 34,  2,  6, 20, 25,  1, 38, 40, 20, 21, 32, 33,  2, 10,\n        34, 22, 34, 10, 35, 12,  2, 31, 40,  2, 10, 34, 33, 35, 10, 34,  1, 38,\n        15, 35, 36, 10,  2, 34, 35,  2, 36, 16, 20,  2, 32, 38, 34, 10,  2, 38,\n        34, 33, 36,  1, 36, 37, 40, 20,  2, 10, 34, 18, 35, 28,  2, 31, 10, 25,\n         2, 28, 34, 39,  2, 10, 34, 13, 38, 10, 34, 40,  1, 38, 34, 10, 16,  2,\n        10, 34, 24, 11, 16,  2, 35, 36,  2, 13, 16, 13,  2, 10, 34, 14, 20, 40,\n        10,  1, 33, 35, 10,  2, 34, 10, 16,  2, 10, 34, 35, 23, 40, 11,  2, 11,\n        28, 40, 36,  2, 32, 10, 34,  1, 31, 40, 10, 34, 34, 37,  2, 33, 35,  2,\n        19, 10, 11, 13,  2, 32, 34, 38, 11,  1])\n","output_type":"stream"}]},{"cell_type":"code","source":"# train/val split=90/10\nn=int(0.9*len(data))\ntrain_data=data[:n]\nval_data=data[n:]","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:42.802980Z","iopub.execute_input":"2024-07-13T15:15:42.803282Z","iopub.status.idle":"2024-07-13T15:15:42.807944Z","shell.execute_reply.started":"2024-07-13T15:15:42.803257Z","shell.execute_reply":"2024-07-13T15:15:42.807085Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# hparams\ndevice='cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)\nbatch_size=64\nblock_size=256\nmax_iters=10000\neval_interval=500\nlr=3e-4\neval_iters=200\nn_embd=384\nn_head=6\nn_layer=6\ndropout=0.2","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:42.811043Z","iopub.execute_input":"2024-07-13T15:15:42.811349Z","iopub.status.idle":"2024-07-13T15:15:42.840394Z","shell.execute_reply.started":"2024-07-13T15:15:42.811326Z","shell.execute_reply":"2024-07-13T15:15:42.839541Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# separate data into batches\n\ndef get_batch(split):\n    data=train_data if split == 'train' else val_data\n    ix=torch.randint(len(data)-block_size,(batch_size,))\n    x=torch.stack([data[i:i+block_size] for i in ix]) \n    y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x,y=x.to(device),y.to(device)\n    return x,y","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:42.841378Z","iopub.execute_input":"2024-07-13T15:15:42.841692Z","iopub.status.idle":"2024-07-13T15:15:42.856852Z","shell.execute_reply.started":"2024-07-13T15:15:42.841659Z","shell.execute_reply":"2024-07-13T15:15:42.856019Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out={}\n    model.eval()\n    for split in ['train','val']:\n        losses=torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X,Y=get_batch(split)\n            logits,loss=model(X,Y)\n            losses[k]=loss.item()\n        out[split]=losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:42.857800Z","iopub.execute_input":"2024-07-13T15:15:42.858029Z","iopub.status.idle":"2024-07-13T15:15:42.867556Z","shell.execute_reply.started":"2024-07-13T15:15:42.858009Z","shell.execute_reply":"2024-07-13T15:15:42.866789Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class Head(nn.Module):\n    # single head of self-attention\n    def __init__(self,head_size):\n        super().__init__()\n        self.key=nn.Linear(n_embd,head_size,bias=False)\n        self.query=nn.Linear(n_embd,head_size,bias=False)\n        self.value=nn.Linear(n_embd,head_size,bias=False)\n        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n        self.dropout=nn.Dropout(dropout)\n    \n    def forward(self,x):\n        B,T,C=x.shape\n        k=self.key(x) # (B,T,C)\n        q=self.query(x) # (B,T,C)\n        wei=q @ k.transpose(-2,-1) * C**-0.5 # (B,T,C) @ (B,C,T) ---> (B,T,T) \n        wei=wei.masked_fill(self.tril[:T,:T]==0,float('-inf')) # (B,T,T)\n        wei=F.softmax(wei,dim=-1) # (B,T,T)\n        wei=self.dropout(wei)\n        v=self.value(x)\n        out=wei @ v # (B,T,T) @ (B,T,C) ---> (B,T,C) \n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:42.868592Z","iopub.execute_input":"2024-07-13T15:15:42.868872Z","iopub.status.idle":"2024-07-13T15:15:42.882788Z","shell.execute_reply.started":"2024-07-13T15:15:42.868850Z","shell.execute_reply":"2024-07-13T15:15:42.882056Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self,num_heads,head_size):\n        super().__init__()\n        self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj=nn.Linear(num_heads * head_size,n_embd)\n        self.dropout=nn.Dropout(dropout)\n        \n    def forward(self,x):\n        out=torch.cat([h(x) for h in self.heads],dim=-1)\n        out=self.dropout(self.proj(out))\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:42.883943Z","iopub.execute_input":"2024-07-13T15:15:42.884218Z","iopub.status.idle":"2024-07-13T15:15:42.893410Z","shell.execute_reply.started":"2024-07-13T15:15:42.884195Z","shell.execute_reply":"2024-07-13T15:15:42.892583Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self,n_embd):\n        super().__init__()\n        self.net=nn.Sequential(\n            nn.Linear(n_embd,4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd,n_embd),\n            nn.Dropout(dropout),\n        )\n        \n        \n    def forward(self,x):\n        return self.net(x)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:42.894283Z","iopub.execute_input":"2024-07-13T15:15:42.894532Z","iopub.status.idle":"2024-07-13T15:15:42.907216Z","shell.execute_reply.started":"2024-07-13T15:15:42.894511Z","shell.execute_reply":"2024-07-13T15:15:42.906356Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n#     Tranformer block: communication (self-attention) followed by computation (feed-forward)\n\n    def __init__(self,n_embd,n_head):\n        super().__init__()\n        head_size=n_embd // n_head\n        self.sa=MultiHeadAttention(n_head,head_size)\n        self.ff=FeedForward(n_embd)\n        self.ln1=nn.LayerNorm(n_embd)\n        self.ln2=nn.LayerNorm(n_embd)\n        \n    def forward(self,x):\n        x=x + self.sa(self.ln1(x))\n        x=x + self.ff(self.ln2(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:42.908371Z","iopub.execute_input":"2024-07-13T15:15:42.908930Z","iopub.status.idle":"2024-07-13T15:15:42.918016Z","shell.execute_reply.started":"2024-07-13T15:15:42.908896Z","shell.execute_reply":"2024-07-13T15:15:42.917282Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class DiwanGPT(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.token_embedding=nn.Embedding(vocab_size,n_embd)\n        self.pos_embedding=nn.Embedding(block_size,n_embd)\n        self.blocks=nn.Sequential(*[Block(n_embd,n_head) for _ in range(n_layer)])\n        self.ln_f=nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head=nn.Linear(n_embd,vocab_size)\n        self.apply(self._init_weights)\n    \n    def _init_weights(self,module):\n        if isinstance(module,nn.Linear):\n            torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module,nn.Embedding):\n            torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n            \n    def forward(self,idx,targets=None):\n        B,T=idx.shape\n        tok_emb=self.token_embedding(idx) # (B,T,C)\n        pos_emb=self.pos_embedding(torch.arange(T,device=device)) # (T,C)\n        # (B,T,C)\n        x=tok_emb + pos_emb \n        x=self.blocks(x)\n        x=self.ln_f(x)\n        logits=self.lm_head(x) # (B,T,vocab_size)\n        if targets is None:\n            loss=None\n        else:\n            B,T,C=logits.shape\n            logits=logits.view(B*T,C)\n            targets=targets.view(B*T)\n            loss=F.cross_entropy(logits,targets)\n        return logits,loss\n    \n    def generate(self,idx,max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond=idx[:,-block_size:]\n            logits,loss=self(idx_cond)\n            logits=logits[:,-1,:] # (B,C)\n            probs=F.softmax(logits,dim=-1)\n#             sample from distribution using probs\n            idx_next=torch.multinomial(probs,num_samples=1) # (B,1)\n            idx=torch.cat((idx,idx_next),dim=1) # (B,T + 1)\n        return idx","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:42.919005Z","iopub.execute_input":"2024-07-13T15:15:42.919304Z","iopub.status.idle":"2024-07-13T15:15:42.932260Z","shell.execute_reply.started":"2024-07-13T15:15:42.919281Z","shell.execute_reply":"2024-07-13T15:15:42.931306Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model=DiwanGPT()\nm=model.to(device)\nprint(sum(p.numel() for p in m.parameters())/1e6,'M parameters') # num of params in model","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:42.933349Z","iopub.execute_input":"2024-07-13T15:15:42.933618Z","iopub.status.idle":"2024-07-13T15:15:43.341672Z","shell.execute_reply.started":"2024-07-13T15:15:42.933590Z","shell.execute_reply":"2024-07-13T15:15:43.340642Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"10.773549 M parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer=torch.optim.AdamW(model.parameters(),lr=lr)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:43.342819Z","iopub.execute_input":"2024-07-13T15:15:43.343095Z","iopub.status.idle":"2024-07-13T15:15:44.627090Z","shell.execute_reply.started":"2024-07-13T15:15:43.343071Z","shell.execute_reply":"2024-07-13T15:15:44.626254Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# train\nstart_time=time.time()\nfor i in range(max_iters):\n    if i % eval_interval == 0 or iter == max_iters - 1:\n        losses=estimate_loss()\n        elapsed_time=time.time() - start_time\n        print(f\"Time elapsed: {str(datetime.timedelta(seconds=int(elapsed_time)))} step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    xb,yb=get_batch('train')\n    logits,loss=model(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:15:44.628414Z","iopub.execute_input":"2024-07-13T15:15:44.629012Z","iopub.status.idle":"2024-07-13T16:13:31.198416Z","shell.execute_reply.started":"2024-07-13T15:15:44.628978Z","shell.execute_reply":"2024-07-13T16:13:31.197496Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Time elapsed: 0:00:32 step 0: train loss 3.8994, val loss 3.8971\nTime elapsed: 0:03:26 step 500: train loss 2.4514, val loss 2.4228\nTime elapsed: 0:06:19 step 1000: train loss 2.1692, val loss 2.1329\nTime elapsed: 0:09:12 step 1500: train loss 2.0350, val loss 2.0049\nTime elapsed: 0:12:06 step 2000: train loss 1.9619, val loss 1.9421\nTime elapsed: 0:14:59 step 2500: train loss 1.9158, val loss 1.8983\nTime elapsed: 0:17:52 step 3000: train loss 1.8819, val loss 1.8681\nTime elapsed: 0:20:46 step 3500: train loss 1.8564, val loss 1.8462\nTime elapsed: 0:23:39 step 4000: train loss 1.8346, val loss 1.8293\nTime elapsed: 0:26:32 step 4500: train loss 1.8190, val loss 1.8134\nTime elapsed: 0:29:26 step 5000: train loss 1.8043, val loss 1.7995\nTime elapsed: 0:32:19 step 5500: train loss 1.7918, val loss 1.7866\nTime elapsed: 0:35:12 step 6000: train loss 1.7782, val loss 1.7782\nTime elapsed: 0:38:05 step 6500: train loss 1.7682, val loss 1.7687\nTime elapsed: 0:40:59 step 7000: train loss 1.7533, val loss 1.7588\nTime elapsed: 0:43:52 step 7500: train loss 1.7435, val loss 1.7478\nTime elapsed: 0:46:45 step 8000: train loss 1.7374, val loss 1.7429\nTime elapsed: 0:49:39 step 8500: train loss 1.7227, val loss 1.7302\nTime elapsed: 0:52:32 step 9000: train loss 1.7151, val loss 1.7225\nTime elapsed: 0:55:25 step 9500: train loss 1.7063, val loss 1.7190\n","output_type":"stream"}]},{"cell_type":"code","source":"# generate\ncontext=torch.zeros((1,1),dtype=torch.long,device=device)\nprint(decode(m.generate(context,max_new_tokens=1000)[0].tolist()))\n#open('/kaggle/working/generated_poetry.txt','w').write(decode(m.generate(context,max_new_tokens=10000)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:13:31.199717Z","iopub.execute_input":"2024-07-13T16:13:31.200006Z","iopub.status.idle":"2024-07-13T16:13:47.191773Z","shell.execute_reply.started":"2024-07-13T16:13:31.199981Z","shell.execute_reply":"2024-07-13T16:13:47.190867Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\tانينا  سيدا\nفي تمرة   وبما نابى\nطير موزنا والشيوخ صادية\n فبكيس \nوكان ايقظواد \nووصف\nكان للبرق أمنهما\nتعتاد بهم  فتى\nهذا الطيور  دون أرتحال صمت النبوءة\nهذه العقار ولن تعقلون لغضبا\nإلى الصيالحون\nأعطيهما  لصدوريه\nطليق أمريك  حيثما تجوز باعتراضاتي كخطآطائك  خطى الطفلة\nتحت الغبار\nمكشوب  يا موتي انضجاع \nتافها  غزلتي  نضحت تموتي  وغبي\nالشواقي  خذي\nخيم الزراعي  لوعة سرقت تسعفي\nأمليكة الظلام  ظل القمر \nكي يدفن بالزيتون\nفترمي كامله  قبي\nعشرين  موت عشرين  والخطر الغريزة\nحتى يقيسون الزيتون\nويسقط ويلقيه\nويدعي أجبتين فمك\nيا كعباس القمر أبحث عني\nأقول كاتب عن فراقها ؟\nونحبتين  \nوقد يبكي غربتي\nإطاروا الصخر من أين دقتنا\nوأجبتين قرما تقرأ\nبكل التقاطع  لكي يخيب\nودمعي سنة التراب يربو\nقد فعلت معرة فأتى\nوحملت بجلجال قمري\nواستقبلته دمعي \nوأتعبتي \nمنشأة حملت أعرضنا\nفبعلمي \nأعيم نهرا من هناك؟\nوسمي \nالقلوب تصوير النار\nوضغطت ركوة الخاصم \nإذن للجلابديو \nأهمي زهرة زهر الغاوي\nوما السمع المروي \nأولادي  \nوألف أهمي وأدري \nمشيرا بالخضم \nوجدا تمادلا عليه \nطيبي \nيا فمي  يا يسرب الغاضة الضرامي\nرعشت يدي \nوأين القرى كهرب \nتنو\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}