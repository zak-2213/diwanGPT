{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8916100,"sourceType":"datasetVersion","datasetId":5356391},{"sourceId":8916111,"sourceType":"datasetVersion","datasetId":5356424}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(44) # better for debugging to have seed \nimport time\nimport datetime\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-11T16:20:00.990291Z","iopub.execute_input":"2024-07-11T16:20:00.990648Z","iopub.status.idle":"2024-07-11T16:20:03.654870Z","shell.execute_reply.started":"2024-07-11T16:20:00.990620Z","shell.execute_reply":"2024-07-11T16:20:03.653629Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/tiny-arabic-poetry/arabic_poems.txt\n/kaggle/input/arabic-poetry/arabic_poems.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('../input/tiny-arabic-poetry/arabic_poems.txt', 'r', encoding='utf-8') as f:\n    poems = f.read()","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:03.661319Z","iopub.execute_input":"2024-07-11T16:20:03.661896Z","iopub.status.idle":"2024-07-11T16:20:03.750733Z","shell.execute_reply.started":"2024-07-11T16:20:03.661858Z","shell.execute_reply":"2024-07-11T16:20:03.749556Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(\"Number of characters in dataset = \", len(poems))","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:03.752350Z","iopub.execute_input":"2024-07-11T16:20:03.752963Z","iopub.status.idle":"2024-07-11T16:20:03.758449Z","shell.execute_reply.started":"2024-07-11T16:20:03.752931Z","shell.execute_reply":"2024-07-11T16:20:03.757416Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Number of characters in dataset =  10000009\n","output_type":"stream"}]},{"cell_type":"code","source":"poems[:1000]","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:03.761642Z","iopub.execute_input":"2024-07-11T16:20:03.761992Z","iopub.status.idle":"2024-07-11T16:20:03.772539Z","shell.execute_reply.started":"2024-07-11T16:20:03.761963Z","shell.execute_reply":"2024-07-11T16:20:03.771591Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"' مولاي قل لي اين ما قد كان من عهد وثيق حاشاك ان تنسي الذي بيني وبينك من حقوق ما مثل وجهك ذا الجمي ل يكون من اهل العقوق يبدو فيشرق لعيو ن ضحي ويشرقني بريقي وزعمت انك زاءري فتركت عيني لطريق وجعلتني ابكي علي ك من الغروب الي الشروق لو ان لي عينا تنا م قنعت بالطيف الطروق سقيا لايام الوصا ل وذلك العيش الانيق\\nرَحِمَ اللَهُ جَعفَراً فَلَقَد كا\\nنَ أَبِيّاً شَهماً وَكانَ رَحيما\\nمُثِّلَ المَوتُ بَينَ عَينَيهِ وَالذُلُّ\\nفَكُلّاً رَآهُ خَطباً عَظيما\\nثُمَّ سارَت بِهِ الحَمِيَّةُ قُدماً\\nفَأَماتَ العِدى وَماتَ كَريما\\nشَهِدتُ بِأَنّي عَبدُ مَغناكُمُ الَّذي\\nعَلى بابِكُم أَرضى حِجابَكُمُ عَنّي\\nفَإِن شَنَّعَ الأَعداءُ عَنّي بِضِدِّهِ\\nفَلا تَشهَدوا إِلّا بِمَسموعِكُم مِنّي\\n\\n قيل لي لم ذمت كل البرايا وهجوت الانام هجوا قبيحا قلت هب اني كذبت عليهم فاروني من يستحق المديحا\\nلعمري يا عميد وأنت أدرى\\nبحال الشعب فينا والحكومه\\nسألناها العدالة فاستبدت\\nبسلطتها وعدتها جريمه\\nفمن باللّه ينسب لاجترام\\nأجنبي إن بقت للحق قيمه\\nمَتى تُلقِ فَودَيها عَلى ظَهرِ ناهِضِ\\n\\nقَفا زيدٍ لقد جرّبت مني\\nأنامل كالسياط ذوات حوم\\nكأنك سيف زيد ال'"},"metadata":{}}]},{"cell_type":"code","source":"# gather all unique characters within dataset\nchars = sorted(list(set(poems)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n# target loss = -ln(1/vocab_size) = -ln(1/55) = 4.007","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:03.773908Z","iopub.execute_input":"2024-07-11T16:20:03.774219Z","iopub.status.idle":"2024-07-11T16:20:04.481520Z","shell.execute_reply.started":"2024-07-11T16:20:03.774193Z","shell.execute_reply":"2024-07-11T16:20:04.480442Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\t\n  ،؛؟ءآأؤإئابةتثجحخدذرزسشصضطظعغـفقكلمنهوىيًٌٍَُِّْچڑک\n55\n","output_type":"stream"}]},{"cell_type":"code","source":"# tokenize chars by mapping to integers\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s] # string --> list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # list of integers --> string","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:04.482742Z","iopub.execute_input":"2024-07-11T16:20:04.483084Z","iopub.status.idle":"2024-07-11T16:20:04.493135Z","shell.execute_reply.started":"2024-07-11T16:20:04.483044Z","shell.execute_reply":"2024-07-11T16:20:04.491916Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\n# encode dataset\ndata = torch.tensor(encode(poems), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000])","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:04.494791Z","iopub.execute_input":"2024-07-11T16:20:04.495230Z","iopub.status.idle":"2024-07-11T16:20:07.385203Z","shell.execute_reply.started":"2024-07-11T16:20:04.495190Z","shell.execute_reply":"2024-07-11T16:20:07.384099Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"torch.Size([10000009]) torch.int64\ntensor([ 2, 38, 41, 37, 13, 43,  2, 35, 37,  2, 37, 43,  2, 13, 43, 39,  2, 38,\n        13,  2, 35, 21,  2, 36, 13, 39,  2, 38, 39,  2, 31, 40, 21,  2, 41, 17,\n        43, 35,  2, 19, 13, 26, 13, 36,  2, 13, 39,  2, 16, 39, 25, 43,  2, 13,\n        37, 22, 43,  2, 14, 43, 39, 43,  2, 41, 14, 43, 39, 36,  2, 38, 39,  2,\n        19, 35, 41, 35,  2, 38, 13,  2, 38, 17, 37,  2, 41, 18, 40, 36,  2, 22,\n        13,  2, 13, 37, 18, 38, 43,  2, 37,  2, 43, 36, 41, 39,  2, 38, 39,  2,\n        13, 40, 37,  2, 13, 37, 31, 35, 41, 35,  2, 43, 14, 21, 41,  2, 34, 43,\n        26, 23, 35,  2, 37, 31, 43, 41,  2, 39,  2, 28, 19, 43,  2, 41, 43, 26,\n        23, 35, 39, 43,  2, 14, 23, 43, 35, 43,  2, 41, 24, 31, 38, 16,  2, 13,\n        39, 36,  2, 24, 13,  7, 23, 43,  2, 34, 16, 23, 36, 16,  2, 31, 43, 39,\n        43,  2, 37, 29, 23, 43, 35,  2, 41, 18, 31, 37, 16, 39, 43,  2, 13, 14,\n        36, 43,  2, 31, 37, 43,  2, 36,  2, 38, 39,  2, 13, 37, 32, 23, 41, 14,\n         2, 13, 37, 43,  2, 13, 37, 26, 23, 41, 35,  2, 37, 41,  2, 13, 39,  2,\n        37, 43,  2, 31, 43, 39, 13,  2, 16, 39, 13,  2, 38,  2, 35, 39, 31, 16,\n         2, 14, 13, 37, 29, 43, 34,  2, 13, 37, 29, 23, 41, 35,  2, 25, 35, 43,\n        13,  2, 37, 13, 43, 13, 38,  2, 13, 37, 41, 27, 13,  2, 37,  2, 41, 22,\n        37, 36,  2, 13, 37, 31, 43, 26,  2, 13, 37, 13, 39, 43, 35,  1, 23, 47,\n        19, 49, 38, 47,  2, 13, 37, 37, 47, 40, 48,  2, 18, 47, 31, 34, 47, 23,\n        13, 44,  2, 34, 47, 37, 47, 35, 47, 21,  2, 36, 13,  1, 39, 47,  2,  9,\n        47, 14, 49, 43, 50, 13, 44,  2, 26, 47, 40, 38, 13, 44,  2, 41, 47, 36,\n        13, 39, 47,  2, 23, 47, 19, 43, 38, 13,  1, 38, 48, 17, 50, 49, 37, 47,\n         2, 13, 37, 38, 47, 41, 16, 48,  2, 14, 47, 43, 39, 47,  2, 31, 47, 43,\n        39, 47, 43, 40, 49,  2, 41, 47, 13, 37, 22, 48, 37, 50, 48,  1, 34, 47,\n        36, 48, 37, 50, 13, 44,  2, 23, 47,  8, 40, 48,  2, 20, 47, 29, 14, 13,\n        44,  2, 31, 47, 30, 43, 38, 13,  1, 17, 48, 38, 50, 47,  2, 25, 13, 23,\n        47, 16,  2, 14, 49, 40, 49,  2, 13, 37, 19, 47, 38, 49, 43, 50, 47, 15,\n        48,  2, 35, 48, 21, 38, 13, 44,  1, 34, 47,  9, 47, 38, 13, 16, 47,  2,\n        13, 37, 31, 49, 21, 42,  2, 41, 47, 38, 13, 16, 47,  2, 36, 47, 23, 43,\n        38, 13,  1, 26, 47, 40, 49, 21, 16, 48,  2, 14, 49,  9, 47, 39, 50, 43,\n         2, 31, 47, 14, 21, 48,  2, 38, 47, 32, 39, 13, 36, 48, 38, 48,  2, 13,\n        37, 50, 47, 22, 43,  1, 31, 47, 37, 42,  2, 14, 13, 14, 49, 36, 48, 38,\n         2,  9, 47, 23, 28, 42,  2, 19, 49, 18, 13, 14, 47, 36, 48, 38, 48,  2,\n        31, 47, 39, 50, 43,  1, 34, 47, 11, 49, 39,  2, 26, 47, 39, 50, 47, 31,\n        47,  2, 13, 37,  9, 47, 31, 21, 13,  7, 48,  2, 31, 47, 39, 50, 43,  2,\n        14, 49, 28, 49, 21, 50, 49, 40, 49,  1, 34, 47, 37, 13,  2, 16, 47, 26,\n        40, 47, 21, 41, 13,  2, 11, 49, 37, 50, 13,  2, 14, 49, 38, 47, 25, 38,\n        41, 31, 49, 36, 48, 38,  2, 38, 49, 39, 50, 43,  1,  1,  2, 35, 43, 37,\n         2, 37, 43,  2, 37, 38,  2, 22, 38, 16,  2, 36, 37,  2, 13, 37, 14, 23,\n        13, 43, 13,  2, 41, 40, 18, 41, 16,  2, 13, 37, 13, 39, 13, 38,  2, 40,\n        18, 41, 13,  2, 35, 14, 43, 19, 13,  2, 35, 37, 16,  2, 40, 14,  2, 13,\n        39, 43,  2, 36, 22, 14, 16,  2, 31, 37, 43, 40, 38,  2, 34, 13, 23, 41,\n        39, 43,  2, 38, 39,  2, 43, 25, 16, 19, 35,  2, 13, 37, 38, 21, 43, 19,\n        13,  1, 37, 31, 38, 23, 43,  2, 43, 13,  2, 31, 38, 43, 21,  2, 41,  9,\n        39, 16,  2,  9, 21, 23, 42,  1, 14, 19, 13, 37,  2, 13, 37, 26, 31, 14,\n         2, 34, 43, 39, 13,  2, 41, 13, 37, 19, 36, 41, 38, 40,  1, 25,  9, 37,\n        39, 13, 40, 13,  2, 13, 37, 31, 21, 13, 37, 15,  2, 34, 13, 25, 16, 14,\n        21, 16,  1, 14, 25, 37, 29, 16, 40, 13,  2, 41, 31, 21, 16, 40, 13,  2,\n        18, 23, 43, 38, 40,  1, 34, 38, 39,  2, 14, 13, 37, 37, 50, 40,  2, 43,\n        39, 25, 14,  2, 37, 13, 18, 16, 23, 13, 38,  1,  9, 18, 39, 14, 43,  2,\n        11, 39,  2, 14, 35, 16,  2, 37, 37, 19, 35,  2, 35, 43, 38, 40,  1, 38,\n        47, 16, 42,  2, 16, 48, 37, 35, 49,  2, 34, 47, 41, 21, 47, 43, 40, 13,\n         2, 31, 47, 37, 42,  2, 30, 47, 40, 23, 49,  2, 39, 13, 40, 49, 28, 49,\n         1,  1, 35, 47, 34, 13,  2, 24, 43, 21, 46,  2, 37, 35, 21,  2, 18, 23,\n        50, 14, 16,  2, 38, 39, 43,  1,  9, 39, 13, 38, 37,  2, 36, 13, 37, 25,\n        43, 13, 29,  2, 22, 41, 13, 16,  2, 19, 41, 38,  1, 36,  9, 39, 36,  2,\n        25, 43, 34,  2, 24, 43, 21,  2, 13, 37])\n","output_type":"stream"}]},{"cell_type":"code","source":"# train/val split = 90/10\n\nn = int(0.9 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:07.386792Z","iopub.execute_input":"2024-07-11T16:20:07.387128Z","iopub.status.idle":"2024-07-11T16:20:07.392439Z","shell.execute_reply.started":"2024-07-11T16:20:07.387098Z","shell.execute_reply":"2024-07-11T16:20:07.391218Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# hparams\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)\nbatch_size = 64\nblock_size = 256\nmax_iters = 5000\neval_interval = 500\nlr = 3e-4\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:21:33.243714Z","iopub.execute_input":"2024-07-11T16:21:33.244479Z","iopub.status.idle":"2024-07-11T16:21:33.251724Z","shell.execute_reply.started":"2024-07-11T16:21:33.244444Z","shell.execute_reply":"2024-07-11T16:21:33.250397Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# separate data into batches\n\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix]) \n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y  = x.to(device), y.to(device)\n    return x,y","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:07.439369Z","iopub.execute_input":"2024-07-11T16:20:07.440295Z","iopub.status.idle":"2024-07-11T16:20:07.449231Z","shell.execute_reply.started":"2024-07-11T16:20:07.440252Z","shell.execute_reply":"2024-07-11T16:20:07.448221Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:07.450717Z","iopub.execute_input":"2024-07-11T16:20:07.451147Z","iopub.status.idle":"2024-07-11T16:20:07.460633Z","shell.execute_reply.started":"2024-07-11T16:20:07.451109Z","shell.execute_reply":"2024-07-11T16:20:07.459718Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class Head(nn.Module):\n    # single head of self-attention\n    \n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        B,T,C = x.shape\n\n        k = self.key(x) # (B, T, C)\n        q = self.query(x) # (B, T, C)\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,C) @ (B,C,T) ---> (B,T,T) \n        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf')) # (B,T,T)\n        wei = F.softmax(wei, dim=-1) # (B,T,T)\n        wei = self.dropout(wei)\n\n        v = self.value(x)\n        out = wei @ v # (B,T,T) @ (B,T,C) ---> (B,T,C) \n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:07.462402Z","iopub.execute_input":"2024-07-11T16:20:07.462826Z","iopub.status.idle":"2024-07-11T16:20:07.475647Z","shell.execute_reply.started":"2024-07-11T16:20:07.462787Z","shell.execute_reply":"2024-07-11T16:20:07.474505Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(num_heads * head_size, n_embd)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:07.480411Z","iopub.execute_input":"2024-07-11T16:20:07.481282Z","iopub.status.idle":"2024-07-11T16:20:07.490964Z","shell.execute_reply.started":"2024-07-11T16:20:07.481240Z","shell.execute_reply":"2024-07-11T16:20:07.489955Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n        \n        \n    def forward(self, x):\n        return self.net(x)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:07.492302Z","iopub.execute_input":"2024-07-11T16:20:07.492735Z","iopub.status.idle":"2024-07-11T16:20:07.503931Z","shell.execute_reply.started":"2024-07-11T16:20:07.492707Z","shell.execute_reply":"2024-07-11T16:20:07.502943Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n#     Tranformer block: communication (self-attention) followed by computation (feed-forward)\n\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ff = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n        \n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ff(self.ln2(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:07.505545Z","iopub.execute_input":"2024-07-11T16:20:07.506010Z","iopub.status.idle":"2024-07-11T16:20:07.516307Z","shell.execute_reply.started":"2024-07-11T16:20:07.505974Z","shell.execute_reply":"2024-07-11T16:20:07.515191Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class DiwanGPT(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n        self.pos_embedding = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            \n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        \n        tok_emb = self.token_embedding(idx) # (B,T,C)\n        pos_emb = self.pos_embedding(torch.arange(T, device=device)) # (T,C)\n        \n        # (B,T,C)\n        x = tok_emb + pos_emb \n        x = self.blocks(x)\n        x = self.ln_f(x)\n        \n        logits = self.lm_head(x) # (B,T,vocab_size)\n        \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:,-block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:,-1,:] # (B,C)\n            probs = F.softmax(logits, dim=-1)\n#             sample from distribution using probs\n            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T + 1)\n        return idx","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:07.517974Z","iopub.execute_input":"2024-07-11T16:20:07.518815Z","iopub.status.idle":"2024-07-11T16:20:07.535368Z","shell.execute_reply.started":"2024-07-11T16:20:07.518782Z","shell.execute_reply":"2024-07-11T16:20:07.534230Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = DiwanGPT()\nm = model.to(device)\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters') # num of params in model","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:07.536600Z","iopub.execute_input":"2024-07-11T16:20:07.536921Z","iopub.status.idle":"2024-07-11T16:20:07.950285Z","shell.execute_reply.started":"2024-07-11T16:20:07.536892Z","shell.execute_reply":"2024-07-11T16:20:07.949159Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"10.781239 M parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=lr)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:20:07.951733Z","iopub.execute_input":"2024-07-11T16:20:07.952168Z","iopub.status.idle":"2024-07-11T16:20:09.061378Z","shell.execute_reply.started":"2024-07-11T16:20:07.952128Z","shell.execute_reply":"2024-07-11T16:20:09.060223Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# train\nstart_time = time.time()\n\nfor i in range(max_iters):\n    if i % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        elapsed_time = time.time() - start_time\n        print(f\"Time elapsed: {str(datetime.timedelta(seconds=int(elapsed_time)))} step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    \n    xb, yb = get_batch('train')\n    \n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-07-11T16:52:24.515815Z","iopub.execute_input":"2024-07-11T16:52:24.516839Z","iopub.status.idle":"2024-07-11T17:21:22.652481Z","shell.execute_reply.started":"2024-07-11T16:52:24.516801Z","shell.execute_reply":"2024-07-11T17:21:22.651352Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Time elapsed: 0:00:32 step 0: train loss 1.7708, val loss 1.8093\nTime elapsed: 0:03:26 step 500: train loss 1.7625, val loss 1.7974\nTime elapsed: 0:06:20 step 1000: train loss 1.7416, val loss 1.7808\nTime elapsed: 0:09:14 step 1500: train loss 1.7314, val loss 1.7738\nTime elapsed: 0:12:08 step 2000: train loss 1.7186, val loss 1.7661\nTime elapsed: 0:15:01 step 2500: train loss 1.7072, val loss 1.7536\nTime elapsed: 0:17:55 step 3000: train loss 1.7014, val loss 1.7515\nTime elapsed: 0:20:49 step 3500: train loss 1.6884, val loss 1.7418\nTime elapsed: 0:23:43 step 4000: train loss 1.6821, val loss 1.7382\nTime elapsed: 0:26:37 step 4500: train loss 1.6735, val loss 1.7332\n","output_type":"stream"}]},{"cell_type":"code","source":"# generate\ncontext = torch.zeros((1,1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))\n#open('/kaggle/working/generated_poetry.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-07-11T17:21:50.090371Z","iopub.execute_input":"2024-07-11T17:21:50.090802Z","iopub.status.idle":"2024-07-11T17:22:10.809078Z","shell.execute_reply.started":"2024-07-11T17:21:50.090765Z","shell.execute_reply":"2024-07-11T17:22:10.807961Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"\tئِلُ\nذَلِلتُ لَهُ بِشراءَ بابِ وَمَعرِكَةٌ\nتَكّو وُليتَها خُدودُ المُنى وَطَعا\nوَهَل سَلَت بِهِ راقَت بِمُخَلَّدِهِ\nمُنسِلُ السُحبِ البَدرِ مِن خَميرانِها\nنَتَيمانِ إِلى إِبلَيهِ لَجاتِهِ\nرَعَت بُغالِ الكَفاحِ الطَريفِ مُرتَصِبا\nوَهَل عَيِّها بِالعَتيقِ باقِياً لِمَن تَرى\nبِيَحيى نَعَمَ السَحاباءَ مالاقِيا\nوَصَوتُ ذُرى صَوتِها وَاتَبَعَ بَعضَةً\nعَرضاً كَأَنّي كانَ أَنّي أَغُمضَبا\nلامَت في مُجاشِبِ النَكبَينِ بادِئاً\nفَمالَمِصرُ نَسينُ الذِراعِ الضَعيفِ\nهَذي حَلَمتِنا الرياحِ فَكَأَنَّما\nحَوامِلَ غُربانا مَواطِنا السيفِ\nإِن كانَتِ اللُجامُ لَها وَفَقَد رَآنا\nعَبُسَ القَبيحَةَ غَمرُ الدُروبِ صِراحاً\nرَجَعنا بِكُلِّ شَيبٍ أَم وَجيدٍ أَقومِ\n\nإنّ سوادي المعذب حدّثنا\nوالذميل ينسانٌ أثواب\nولكن أهجز الشبابُ والشب\nبانوا علي ثوب الغيب لأدم\nفي دولة الأطرار يا ملاذوني\nملك نار الكشاف بعدها وليد\nقطعنا لهم ثم تاه لأنساب\nإذا الذي كرمنا جنى دونكما\nتطلع منها به المورود الجاري\nفاطلب رحمة من تغاب لذيذ\nأحنا مليكل العيد ذي أحباب\nحتى يسخط العب رسما قبرا\nورؤيتها الاجفان ما زندنا\nلكتافء عنا بعد نقع الشمائل\nمتى\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}