diwanGPT was built by following Andrej Karpathy's tutorial on building GPT (link below).
It is a small character-level language model (~10 million parameters) trained on over 200K lines of Arabic poetry that generates its own Arabic poetry. The poetry is not fully coherent but it is pretty good for its size.
It took around an hour to train using an Nvidia P100 GPU on Kaggle.

Video link: https://youtu.be/kCc8FmEb1nY?feature=shared
